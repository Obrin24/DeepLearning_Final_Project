{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import measure\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and split into training, validation and testing datasets\n"
     ]
    }
   ],
   "source": [
    "%run \"../DL_notebooks/src/dataloading.ipynb\"\n",
    "%run \"../DL_notebooks/src/training.ipynb\"\n",
    "%run \"../DL_notebooks/src/evaluate.ipynb\"\n",
    "%run \"../DL_notebooks/src/visualizations.ipynb\"\n",
    "%run \"../DL_notebooks/model_architectures.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Making for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradCAM initialization\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.model.eval()\n",
    "        self.features_map = None\n",
    "        self.grad = None\n",
    "        self.hook = self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.features_map = output\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.grad = grad_out[0]\n",
    "\n",
    "        return [\n",
    "            self.target_layer.register_forward_hook(forward_hook),\n",
    "            self.target_layer.register_backward_hook(backward_hook),\n",
    "        ]\n",
    "\n",
    "    def generate(self, input_tensor, class_idx):\n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(outputs)\n",
    "        one_hot[:, class_idx] = 1.0\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.model(input_tensor)\n",
    "        target = torch.sum(one_hot * outputs)\n",
    "\n",
    "        # Backward pass\n",
    "        target.backward(retain_graph=True)\n",
    "\n",
    "        # Grad-CAM calculation\n",
    "        weights = torch.mean(self.grad, dim=(2, 3), keepdim=True)\n",
    "        gradcam_map = torch.sum(weights * self.features_map, dim=1, keepdim=True)\n",
    "        gradcam_map = nn.functional.relu(gradcam_map)\n",
    "\n",
    "        return gradcam_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabeb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabeb\\Documents\\GitHub\\DeepLearning_Final_Project\\DL_notebooks\\tumor_detection.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MRI_ResNet2().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# GradCAM initialization\n",
    "gradcam_layer = model.model.layer4[-1]  # Adjust the layer based on your model architecture\n",
    "gradcam = GradCAM(model, target_layer=gradcam_layer)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(\"Epoch \", epoch)\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Function to overlay GradCAM map on input images\n",
    "def overlay_gradcam(original_images, gradcam_map):\n",
    "    alpha = 0.5  # Set the transparency of the overlay\n",
    "\n",
    "    for i in range(original_images.shape[0]):\n",
    "        img = original_images[i].numpy().transpose((1, 2, 0))  # Assuming images are in the shape (C, H, W)\n",
    "        gradcam_img = gradcam_map[i, 0].detach().cpu().numpy()\n",
    "\n",
    "        # Normalize GradCAM map to [0, 1]\n",
    "        gradcam_img = (gradcam_img - np.min(gradcam_img)) / (np.max(gradcam_img) - np.min(gradcam_img) + 1e-8)\n",
    "\n",
    "        # Overlay GradCAM map on the original image\n",
    "        overlaid_img = alpha * gradcam_img + (1 - alpha) * img\n",
    "\n",
    "        # Display the original image, GradCAM map, and overlaid image\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(gradcam_img, cmap='jet')\n",
    "        plt.title('GradCAM Map')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(overlaid_img)\n",
    "        plt.title('Overlaid Image')\n",
    "        \n",
    "        plt.show()\n",
    "# Inference and visualization using Grad-CAM\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for inputs, labels in val_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Use GradCAM to visualize tumor localization\n",
    "    gradcam_map = gradcam.generate(inputs, class_idx=0)  # Adjust class_idx based on your classes\n",
    "\n",
    "    # Overlay gradcam_map on the input images for visualization\n",
    "    overlay_gradcam(inputs, gradcam_map)\n",
    "\n",
    "    # Save predictions and labels for evaluation\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Classification Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tumor Localization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabeb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabeb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabeb\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabeb\\Documents\\GitHub\\DeepLearning_Final_Project\\DL_notebooks\\tumor_detection.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m# Use GradCAM to visualize tumor localization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     gradcam_map \u001b[39m=\u001b[39m generate_gradcam(model, inputs, target_layer)  \u001b[39m# Implement generate_gradcam function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m# You can overlay gradcam_map on the input images for visualization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Evaluate the model's classification accuracy and localization accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# You can use standard evaluation metrics like accuracy, precision, recall, and F1-score.\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\gabeb\\Documents\\GitHub\\DeepLearning_Final_Project\\DL_notebooks\\tumor_detection.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Compute gradients of the output class score with respect to feature maps\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m output[:, predicted_class]\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Get the gradients from the target layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m gradients \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_activation_gradients(target_layer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    235\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[0;32m    236\u001b[0m     (inputs,)\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    243\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 244\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         )\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating_point:\n\u001b[0;32m    121\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from gradcam import GradCAM  # Implement GradCAM for PyTorch model\n",
    "\n",
    "class MRI_ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MRI_ClassificationModel, self).__init__()\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        self.target_layer = self.resnet.layer4[-1]  # Define the target layer\n",
    "\n",
    "        # Modify the fully connected layer for classification\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Data preprocessing and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Assume you have predefined data loaders (train_loader, val_loader, test_loader)\n",
    "\n",
    "# Create and train your model\n",
    "num_classes = 4  # Adjust this according to your dataset\n",
    "model = MRI_ClassificationModel(num_classes)\n",
    "torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Choose the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Inference and visualization using Grad-CAM\n",
    "model.eval()\n",
    "gradcam = GradCAM(model, target_layer=model.resnet.layer4[-1])  # Define target_layer appropriately\n",
    "target_layer = model.target_layer\n",
    "for inputs, labels in val_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    # Use GradCAM to visualize tumor localization\n",
    "    gradcam_map = generate_gradcam(model, inputs, target_layer)  # Implement generate_gradcam function\n",
    "    # You can overlay gradcam_map on the input images for visualization\n",
    "\n",
    "# Evaluate the model's classification accuracy and localization accuracy\n",
    "# You can use standard evaluation metrics like accuracy, precision, recall, and F1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# You need to define a method in your model to get activation gradients from the target layer\n",
    "# In this example, we assume you have a `get_activation_gradients` method in your model.\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "        # Register a hook to capture gradients from the target layer\n",
    "        self.target_layer.register_forward_hook(self.save_gradients)\n",
    "\n",
    "    def save_gradients(self, module, input, output):\n",
    "        self.gradients = output[0]\n",
    "\n",
    "    def get_gradcam(self, input_image):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_image)\n",
    "\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0][output.argmax()] = 1\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        activations = self.gradients\n",
    "        weight = F.adaptive_avg_pool2d(activations, 1)\n",
    "        gradcam_map = (activations * weight).sum(dim=1, keepdim=True)\n",
    "\n",
    "        gradcam_map = F.relu(gradcam_map)\n",
    "\n",
    "        return gradcam_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def localization_accuracy_function(gradcam_map, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate localization accuracy based on the Grad-CAM map.\n",
    "\n",
    "    Parameters:\n",
    "    - gradcam_map: Torch tensor containing Grad-CAM maps for each image.\n",
    "    - labels: True labels for each image.\n",
    "    - threshold: Threshold value for binarizing the Grad-CAM map.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Localization accuracy.\n",
    "    \"\"\"\n",
    "    binary_map = (gradcam_map > threshold).float()\n",
    "    correct_predictions = torch.sum(binary_map * labels)\n",
    "    total_predictions = torch.sum(binary_map)\n",
    "\n",
    "    if total_predictions == 0:\n",
    "        return 0.0\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy.item()\n",
    "\n",
    "\n",
    "def overlay_heatmap(image, heatmap, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Overlay the heatmap on the input image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image.\n",
    "    - heatmap: Grad-CAM heatmap.\n",
    "    - alpha: Weight for blending the heatmap with the image.\n",
    "\n",
    "    Returns:\n",
    "    - overlay: Image with overlaid heatmap.\n",
    "    \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(cv2.cvtColor(image, cv2.COLOR_RGB2BGR), 1 - alpha, heatmap, alpha, 0)\n",
    "\n",
    "    return cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MRI_ClassificationModel' object has no attribute 'target_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabeb\\Documents\\GitHub\\DeepLearning_Final_Project\\DL_notebooks\\tumor_detection.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Generate Grad-CAM for the images\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m gradcam_map \u001b[39m=\u001b[39m generate_gradcam(model, images, target_layer\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mtarget_layer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Get the predicted class\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabeb/Documents/GitHub/DeepLearning_Final_Project/DL_notebooks/tumor_detection.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m _, predicted_class \u001b[39m=\u001b[39m model(images)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MRI_ClassificationModel' object has no attribute 'target_layer'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Lists to store evaluation results\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "localization_accuracy = []\n",
    "\n",
    "num_examples_to_visualize = 5  # Define how many examples you want to visualize\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Generate Grad-CAM for the images\n",
    "        gradcam_map = generate_gradcam(model, images, target_layer=model.target_layer)\n",
    "\n",
    "        # Get the predicted class\n",
    "        _, predicted_class = model(images).max(1)\n",
    "\n",
    "        # Calculate classification accuracy\n",
    "        accuracy = accuracy_score(labels.cpu(), predicted_class.cpu())\n",
    "        \n",
    "        # Calculate localization accuracy\n",
    "        localization_accuracy.append(localization_accuracy_function(gradcam_map, labels, threshold=0.5))\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted_class.cpu().numpy())\n",
    "\n",
    "        if i < num_examples_to_visualize:\n",
    "            # Visualize the images with Grad-CAM\n",
    "            for j in range(images.size(0)):\n",
    "                img = images[j].to(device).numpy().transpose(1, 2, 0)\n",
    "                heatmap = gradcam_map[j, 0].to(device).numpy()\n",
    "                heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "                heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))  # Normalize the heatmap\n",
    "                overlay = overlay_heatmap(img, heatmap)\n",
    "\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.subplot(num_examples_to_visualize, images.size(0), i * images.size(0) + j + 1)\n",
    "                plt.title(f'True: {labels[j].item()}, Predicted: {predicted_class[j].item()}')\n",
    "                plt.imshow(overlay)\n",
    "                plt.axis('off')\n",
    "\n",
    "# Calculate other metrics (e.g., precision, recall, F1-score) using true_labels and predicted_labels\n",
    "\n",
    "# Calculate mean localization accuracy\n",
    "mean_localization_accuracy = sum(localization_accuracy) / len(localization_accuracy)\n",
    "\n",
    "# Print and/or log the evaluation metrics\n",
    "print(f'Classification Accuracy: {accuracy}')\n",
    "print(f'Mean Localization Accuracy: {mean_localization_accuracy}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
